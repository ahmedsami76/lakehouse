{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b6856f-206b-4b55-ae40-9c9976f30912",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from delta import *\n",
    "\n",
    "builder = pyspark.sql.SparkSession.builder.appName(\"MyApp\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b229334-6772-49c9-b48e-349486c947f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the DF\n",
    "\n",
    "bike_df = (spark\n",
    "           .read          \n",
    "           .option(\"header\", True)\n",
    "           .option(\"inferSchema\", True)\n",
    "           .option(\"delimiter\", \",\")\n",
    "            .csv(\"file:///data/bike/bike-retail.csv\"))\n",
    "bike_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d4602d-5d8e-4834-8477-c2b800669760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persist as delta table\n",
    "\n",
    "bike_df.write.format(\"delta\").save(\"file:///data/bike/retail-bike\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c51bb8f-ea5a-4753-b2ed-f3e08ae71774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load DF from delta table\n",
    "\n",
    "ddf = spark.read.format(\"delta\").load(\"file:///data/bike/retail-bike\")\n",
    "ddf.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f53a90e-1539-4b9c-a90b-fbe9de6ed6fc",
   "metadata": {},
   "source": [
    "# Essential Delta Lake Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684900c5-2253-4138-ac18-546fd2efe803",
   "metadata": {},
   "source": [
    "## Create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3491d9-40dc-4030-9907-db5bbbfba770",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a80de4-10da-4934-a5c6-d6bb6548d95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a database for the examples\n",
    "spark.sql(\"create database if not exists exampleDB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6927672d-d611-45e5-8fdd-40c9b3ec4358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the table doesn't already exist\n",
    "spark.sql(\"\"\"\n",
    "DROP TABLE if exists exampleDB.countries\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67542f24-e3e4-4ff9-8c57-a0c8c01b1e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty table using sql\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE exampleDB.scountries (\n",
    "  id LONG,\n",
    "  country STRING,\n",
    "  capital STRING\n",
    ") USING DELTA;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5bf1e0-5f32-47ec-8d32-47eabae834c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DeltaTable object using python \n",
    "from pyspark.sql.types import *\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "delta_table = (\n",
    "    DeltaTable.create(spark)\n",
    "    .tableName(\"exampleDB.pcountries\")\n",
    "    .addColumn(\"id\", dataType=LongType(), nullable=False)\n",
    "    .addColumn(\"country\", dataType=StringType(), nullable=False)\n",
    "    .addColumn(\"capital\", dataType=StringType(), nullable=False)\n",
    "    .execute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b48b6e-6a9f-4857-b64e-40f1c1ec1c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data using insert into\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    INSERT INTO exampleDB.scountries VALUES\n",
    "    (1, 'United Kingdom', 'London'),\n",
    "    (2, \"Canada\", \"Ottawa\");\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b14bd6-5805-4255-aafd-aa90f212c10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data using python\n",
    "\n",
    "data = [\n",
    "(1, \"United Kingdom\", \"London\"),\n",
    "(2, \"Canada\", \"Toronto\")\n",
    "]\n",
    "\n",
    "# Create a schema\n",
    "schema = [\"id\", \"country\", \"capital\"]\n",
    "\n",
    "# Create a dataframe\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "df.write.format(\"delta\").insertInto(\"exampleDB.pcountries\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e864cfa3-84c3-466f-8626-798e093b5d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append\n",
    "\n",
    "data = [(3, \"United States\", \"Washington, D.C.\")]\n",
    "\n",
    "schema = [\"id\", \"country\", \"capital\"]\n",
    "\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "df.write.format(\"delta\").mode(\"append\").saveAsTable(\"exampleDB.pcountries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df41c3d5-f4d3-4f74-b7c5-e1ca6da79706",
   "metadata": {},
   "source": [
    "## Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95ce5a9-08a6-498a-9ec2-4cd8af779db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read\n",
    "\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "delta_table = DeltaTable.forName(spark, \"exampleDB.pcountries\")\n",
    "delta_table_df = delta_table.toDF()\n",
    "\n",
    "delta_table_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0cb2c4-7ff0-491b-9b7e-66a50ef3e33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_table_df.select(\"id\", \"capital\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e645f438-d859-44bb-97d6-f5be230845cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering\n",
    "\n",
    "delta_table_df.filter(delta_table_df.capital == 'London').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55027c70-1f17-48cc-83c6-4c14f9aeed75",
   "metadata": {},
   "source": [
    "### Reading with Time Travel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2567ad6d-7d68-46ef-b35c-0f20689e8c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  3|\n",
      "|  1|\n",
      "|  2|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Supported using one of qualifiers: _VERSION AS OF_ and _TIMESTAMP AS OF_\n",
    "# Python - Notice that the DeltaTable API doesn't support time travel read yet\n",
    "\n",
    "# VERSION AS OF 1 show version 1 of the table\n",
    "\n",
    "df = (spark.read.option(\"versionAsOf\", \"1\")\n",
    "         .load(\"file:///data/spark-warehouse/exampledb.db/pcountries\")\n",
    "         .select(\"id\")\n",
    "         .distinct())\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e1b3bac2-35bd-44a8-bca0-a241b9be6c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  1|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Supported using one of qualifiers: _VERSION AS OF_ and _TIMESTAMP AS OF_\n",
    "# Sql\n",
    "\n",
    "# VERSION AS OF 1 show version 1 of the table\n",
    "df = spark.sql(\"SELECT DISTINCT id FROM exampleDB.scountries VERSION AS OF 1\")\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6075ca95-e194-4d97-bdea-2d6dd0d1a98f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  3|\n",
      "|  1|\n",
      "|  2|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Supported using one of qualifiers: _VERSION AS OF_ and _TIMESTAMP AS OF_\n",
    "# Python - Notice that the DeltaTable API doesn't support time travel read yet\n",
    "\n",
    "# TIMESTAMP AS OF <date> shows the number of records before the given date\n",
    "\n",
    "df = (spark.read.option(\"timestampAsOf\", \"2025-01-01\")\n",
    "         .load(\"file:///data/spark-warehouse/exampledb.db/pcountries\")\n",
    "         .select(\"id\")\n",
    "         .distinct())\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9e627387-e288-4aa6-9345-bb7186298947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Supported using one of qualifiers: _VERSION AS OF_ and _TIMESTAMP AS OF_\n",
    "# Sql\n",
    "\n",
    "# TIMESTAMP AS OF <date> shows the number of records before the given date\n",
    "df = spark.sql(\"SELECT DISTINCT id FROM exampleDB.scountries TIMESTAMP AS OF '2025-01-02 04:30'\")\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5c4d14-f76b-4603-bf5c-93744f81b5a6",
   "metadata": {},
   "source": [
    "## Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "afd7e060-7ba6-45db-aaa4-9e7cff1ad823",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[num_affected_rows: bigint]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Update sql\n",
    "\n",
    "spark.sql(\"UPDATE exampleDB.scountries SET country = 'U.K.' WHERE id = 1;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "19bdfe1e-39fc-4285-afeb-23ee1605a9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update python\n",
    "\n",
    "delta_table.update(\n",
    "    condition = \"id = 1\",\n",
    "    set = { \"country\": \"'U.K.'\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611b9616-7ae3-4fa9-926b-32fc51807687",
   "metadata": {},
   "source": [
    "## Delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fd2b0b5c-9f21-4b80-9dab-3754cb035e65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[num_affected_rows: bigint]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Delete sql\n",
    "\n",
    "spark.sql(\"DELETE FROM exampleDB.scountries WHERE id = 1;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f6a811a6-78d0-4c31-a078-04eac89d4140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete python\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "delta_table.delete(\"id = 1\") # Using SQL Expression\n",
    "delta_table.delete(col(\"id\") == 2) # Using PySpark "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bbc710-d6e9-435d-8d00-88fdb52c7a41",
   "metadata": {},
   "source": [
    "## Overwriting Data in a Delta Lake Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e28a23fa-8193-4863-a3e4-48f83959b0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using overwrite mode\n",
    "# python\n",
    "\n",
    "spark.createDataFrame([(1, 'India', 'New Delhi'),(4, 'Australia', 'Canberra'), (3, 'U.S.', 'Washington, D.C.')], schema=[\"id\", \"country\", \"capital\"]) \\\n",
    ".write \\\n",
    ".format(\"delta\") \\\n",
    ".mode(\"overwrite\") \\\n",
    ".saveAsTable(\"exampleDB.pcountries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "617511cd-d477-45f0-aee0-6dbfe7ea25df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using INSERT OVERWRITE\n",
    "# sql\n",
    "\n",
    "spark.sql(\"INSERT OVERWRITE exampleDB.scountries VALUES (3, 'U.S.', 'Washington, D.C.'), (1, 'India', 'New Delhi'), (4, 'Australia', 'Canberra')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a84b1e1-82bd-4a6a-be2c-1888ec0b9509",
   "metadata": {},
   "source": [
    "## Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5bb8cb5f-1d77-41cb-ab3d-157b2567e28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge\n",
    "# python\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "delta_table = DeltaTable.forName(spark, \"exampleDB.pcountries\")\n",
    "\n",
    "idf = (\n",
    "    spark\n",
    "    .createDataFrame([(1, 'India', 'New Delhi'),(4, 'Australia', 'Canberra'), (3, 'U.S.', 'Washington, D.C.')], schema=[\"id\", \"country\", \"capital\"])\n",
    ")\n",
    "\n",
    "delta_table.alias(\"target\").merge(\n",
    "    source = idf.alias(\"source\"),\n",
    "    condition = \"source.id = target.id\"\n",
    ").whenMatchedUpdate(set = {\"country\": \"source.country\", \"capital\": \"source.capital\"}\n",
    "                   ).whenNotMatchedInsert(values = {\"id\": \"source.id\", \"country\": \"source.country\", \"capital\": \"source.capital\"}\n",
    "                                         ).execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896a56c5-69b7-4f57-8d0c-3b1c773fa887",
   "metadata": {},
   "source": [
    "## Delta Lake Metadata and History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "41c54011-788c-4c50-8d5a-d6e8d667e9a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[version: bigint, timestamp: timestamp, userId: string, userName: string, operation: string, operationParameters: map<string,string>, job: struct<jobId:string,jobName:string,jobRunId:string,runId:string,jobOwnerId:string,triggerType:string>, notebook: struct<notebookId:string>, clusterId: string, readVersion: bigint, isolationLevel: string, isBlindAppend: boolean, operationMetrics: map<string,string>, userMetadata: string, engineInfo: string]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta_table.history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5e5832c2-f908-48d1-a4b7-5ab23a6c6aca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[format: string, id: string, name: string, description: string, location: string, createdAt: timestamp, lastModified: timestamp, partitionColumns: array<string>, clusteringColumns: array<string>, numFiles: bigint, sizeInBytes: bigint, properties: map<string,string>, minReaderVersion: int, minWriterVersion: int, tableFeatures: array<string>]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta_table.detail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7e818c13-1088-4ce0-8345-7b434ea61aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----------------+\n",
      "| id|  country|         capital|\n",
      "+---+---------+----------------+\n",
      "|  1|    India|       New Delhi|\n",
      "|  3|     U.S.|Washington, D.C.|\n",
      "|  4|Australia|        Canberra|\n",
      "+---+---------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "delta_table.toDF().show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
